{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# import libraries for ProjectGurukul Credit Card Fraud Detection Project using Machine Learning:\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# import libraries for ProjectGurukul Credit Card Fraud Detection Project using Machine Learning:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix , accuracy_score, classification_report\n",
    "#Loading the dataset to a Pandas Dataframe\n",
    "credit_card_data = pd.read_csv(r'creditcard.csv')\n",
    "# let's see first 5 rows of the dataset:\n",
    "credit_card_data.head(5)\n",
    "# let's see last 5 rows of our dataset: \n",
    "credit_card_data.tail()\n",
    "# dataset information: \n",
    "credit_card_data.info()\n",
    "# checking number of missing values:inn\n",
    "credit_card_data.isnull().sum()\n",
    "# Find distribution of Normal transaction or Fraud transaction:\n",
    "credit_card_data['Class'].value_counts()\n",
    "# Separating the data:\n",
    "normal = credit_card_data[credit_card_data.Class == 0]\n",
    "fraud = credit_card_data[credit_card_data.Class == 1]\n",
    "# check shape\n",
    "print(normal.shape)\n",
    "print(fraud.shape)\n",
    "#visualize the data:\n",
    "labels = [\"Normal\", \"Fraud\"]\n",
    "count_classes = credit_card_data.value_counts(credit_card_data['Class'], sort= True)\n",
    "count_classes.plot(kind = \"bar\", rot = 0)\n",
    "plt.title(\"ProjectGurukul\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(range(2), labels)\n",
    "plt.show()\n",
    "# statistical measures of the data:\n",
    "normal.Amount.describe()\n",
    "fraud.Amount.describe()\n",
    "# visualize the data using seaborn:\n",
    "sns.relplot(x = 'Amount' , y = 'Time' , hue = 'Class', data = credit_card_data)\n",
    "# Compare values of both transactions:\n",
    "credit_card_data.groupby('Class').mean()\n",
    "# Now we will build a sample dataset containing similar distribution of normal transaction and fraud transaction:\n",
    "normal_sample = normal.sample(n=492)\n",
    "# Concat two data ( normal_sample and fraud) to create new dataframe which consist equal number of fraud transactions and normal transactions, In this way we balance our dataset (As our dataset is highly unbalanced initially) :\n",
    "credit_card_new_data = pd.concat([normal_sample, fraud], axis=0)\n",
    "# Letâ€™s see our new dataset:\n",
    "credit_card_new_data\n",
    "# Analyse our new dataset:\n",
    "credit_card_new_data['Class'].value_counts()\n",
    "# Splitting data into features and targetsE\n",
    "X = credit_card_new_data.drop('Class', axis=1)\n",
    "Y = credit_card_new_data['Class']\n",
    "# splitting the data into training and testing data:\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, stratify = Y, random_state= 2)\n",
    "print(X.shape, X_train.shape, X_test.shape)\n",
    "# Creating Model:\n",
    "model = LogisticRegression()\n",
    "# training the Logistic Regression model with training data:\n",
    "model.fit(X_train,Y_train)\n",
    "# Model Evaluation\n",
    "X_train_pred = model.predict(X_train)\n",
    "training_data_accuracy = accuracy_score(X_train_pred, Y_train)\n",
    "print('Accuracy of Training data:', training_data_accuracy)\n",
    "# classification report of the model on training data:\n",
    "print(classification_report(X_train_pred, Y_train))\n",
    "# accuracy on test data:\n",
    "X_test_pred = model.predict(X_test)\n",
    "test_data_accuracy = accuracy_score(X_test_pred, Y_test)\n",
    "print('Accuracy of Testing data:', test_data_accuracy)\n",
    "# confusion matrix and classification report of test data:\n",
    "print(confusion_matrix(X_test_pred, Y_test))\n",
    "print(classification_report(X_test_pred,Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
